{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression, RFECV, SelectFromModel\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate Google Drive shareable download links\n",
    "def gd_path(file_id):\n",
    "    \"\"\"Generate a shareable link from Google Drive file id.\"\"\"\n",
    "    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "# Dictionary containing Google Drive file IDs for different data files\n",
    "files_id = {\n",
    "    'housing_data': \"1kX_jcLeBpBGvTo8FXDeU2MK-aw1a0voU\",  # ID for iteration7 data\n",
    "    'test_housing_data': \"1CMsAWhWKWBjI6DDEHtcYmRVZRazfE9bo\",  # ID for test data for housing\n",
    "    'ids_com': \"10gwiL49calkj-xbx-3rQEK4H2zoJcU11\"  # ID for committing the project\n",
    "}\n",
    "\n",
    "# Load the 'housing_data' CSV file using the generated link\n",
    "housing_data = pd.read_csv(gd_path(files_id['housing_data']), sep=\",\")\n",
    "\n",
    "# Load the 'test_housing_data' CSV file using the generated link\n",
    "test_housing_data = pd.read_csv(gd_path(files_id['test_housing_data']), sep=\",\")\n",
    "\n",
    "# Load the 'ids_com' CSV file using the generated link\n",
    "ids_com = pd.read_csv(gd_path(files_id['ids_com']), sep=\",\")\n",
    "\n",
    "# Create a DataFrame 'df' and assign it the 'housing_data' for further processing\n",
    "df = housing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X and y creation\n",
    "y = df.pop(\"SalePrice\")\n",
    "\n",
    "# Feature Engineering\n",
    "X = df\n",
    "\n",
    "# data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical features from X\n",
    "X_cat = X.select_dtypes(exclude=\"number\").copy()\n",
    "X_num = X.select_dtypes(include=\"number\").copy()\n",
    "\n",
    "# Define a pipeline for numerical features\n",
    "numeric_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\")\n",
    ")\n",
    "\n",
    "# Define columns that are treated as ordinal\n",
    "ordinal_cols = X_cat.columns.get_indexer(['LandContour', 'LandSlope',\n",
    "                     'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n",
    "                     'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                     'HeatingQC', 'KitchenQual', 'Functional',\n",
    "                     'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                     'PoolQC'])\n",
    "\n",
    "# Define columns that are one-hot encoded\n",
    "onehot_cols = X_cat.columns.get_indexer(['MSZoning', 'Condition1', 'Heating', 'Street', 'CentralAir', \n",
    "                                         'Foundation', 'Alley', 'LotShape', 'Utilities', 'LotConfig', \n",
    "                                         'Neighborhood', 'Condition2', 'BldgType', 'HouseStyle', \n",
    "                                         'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', \n",
    "                                         'Electrical', 'GarageType', 'PavedDrive', 'Fence', 'MiscFeature',\n",
    "                                         'SaleType', 'SaleCondition'])\n",
    "\n",
    "\n",
    "\n",
    "# Define the categories for ordinal features\n",
    "# These categories are manually established, including \"N_A\"\n",
    "ordinal_categories = [['Lvl', 'Bnk', 'HLS', 'Low', 'N_A'],\n",
    "                      ['Gtl', 'Mod', 'Sev', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Gd', 'Av', 'Mn', 'No', 'N_A'],\n",
    "                      ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'N_A'],\n",
    "                      ['GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Fin', 'RFn', 'Unf', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'N_A'],\n",
    "                      ['Ex', 'Gd', 'TA', 'Fa', 'N_A']]\n",
    "\n",
    "# Create a categorical encoder using ColumnTransformer\n",
    "categorical_encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat_ordinal\", OrdinalEncoder(categories=ordinal_categories), ordinal_cols),\n",
    "        (\"cat_onehot\", OneHotEncoder(handle_unknown=\"ignore\"), onehot_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create pipelines for numerical and categorical features\n",
    "\n",
    "# Pipeline for numerical features\n",
    "numeric_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),          # Impute missing values with the mean\n",
    "    SelectKBest(score_func=f_regression, k=15),  # Select the top 15 features using f_regression\n",
    "    MinMaxScaler(),                         # Scale features to a specified range (0 to 1 by default)\n",
    "    VarianceThreshold(threshold=0.005)      # Remove low-variance features\n",
    ")\n",
    "\n",
    "# Pipeline for categorical features\n",
    "categoric_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"N_A\"),  # Impute missing values with 'N_A'\n",
    "    categorical_encoder  # Apply the categorical encoder created above\n",
    ")\n",
    "\n",
    "# Create a preprocessor that applies the appropriate pipeline to each feature type using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_pipe\", numeric_pipe, X_num_train.columns),  # Apply the numeric pipeline to numerical features\n",
    "        (\"cat_pipe\", categoric_pipe, X_train.columns)    # Apply the categorical pipeline to all features\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisiontree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full pipeline that includes preprocessing steps and a Decision Tree Classifier\n",
    "full_pipeline = make_pipeline(preprocessor, DecisionTreeClassifier())\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"columntransformer__num_pipe__simpleimputer__strategy\": [\"mean\"],  # Impute missing values with the mean\n",
    "    \"decisiontreeclassifier__max_depth\": [18],  # Maximum depth of the decision tree\n",
    "    \"decisiontreeclassifier__min_samples_leaf\": [10]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object to search for the best hyperparameters\n",
    "search = GridSearchCV(full_pipeline, param_grid, cv=5, verbose=1)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best cross-validation score from the grid search for Decision Tree Classifier\n",
    "best_score_dtree = search.best_score_\n",
    "\n",
    "# Store the best score in a dictionary\n",
    "scores = {\"dtree\": best_score_dtree}\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best parameters: \", search.best_params_)\n",
    "print(\"Best score:\", best_score_dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisiontree - Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.168\n",
      "Testing Accuracy: 0.01 \n",
      "\n",
      "R-squared: 0.392\n",
      "MAPE: 0.177\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the training and testing datasets using the best Decision Tree Classifier model\n",
    "y_train_pred = search.predict(X_train)\n",
    "y_test_pred = search.predict(X_test)\n",
    "\n",
    "# Create confusion matrices for training and testing datasets\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "confusion_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Calculate accuracy for training and testing datasets\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the training and testing accuracies\n",
    "print(\"Training Accuracy:\", round(accuracy_train, 3))\n",
    "print(\"Testing Accuracy:\", round(accuracy_test, 3), \"\\n\")\n",
    "\n",
    "# Calculate R-squared (coefficient of determination) for Decision Tree Classifier predictions\n",
    "dtree_r2 = r2_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "# Print the R-squared score\n",
    "print(\"R-squared:\", round(dtree_r2, 3))\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for Decision Tree Classifier predictions\n",
    "dtree_mape = mean_absolute_percentage_error(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "# Print the MAPE score\n",
    "print(\"MAPE:\", round(dtree_mape, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full pipeline that includes preprocessing steps and a K-Nearest Neighbors (KNN) classifier\n",
    "full_pipeline = make_pipeline(preprocessor, KNeighborsClassifier())\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"columntransformer__num_pipe__simpleimputer__strategy\": [\"mean\"],  # Impute missing values with the mean\n",
    "    \"kneighborsclassifier__n_neighbors\": range(2, 29, 2)  # Range of values for the number of neighbors\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object to search for the best hyperparameters\n",
    "search2 = GridSearchCV(full_pipeline, param_grid, cv=10, verbose=1)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "search2.fit(X_train, y_train)\n",
    "\n",
    "# Get the best cross-validation score from the grid search for KNN classifier\n",
    "best_score_knn = search2.best_score_\n",
    "\n",
    "# Store the best score in a dictionary\n",
    "scores2 = {\"knn\": best_score_knn}\n",
    "\n",
    "# Print the best parameters and corresponding score\n",
    "print(\"Best parameters: \", search2.best_params_)\n",
    "print(\"Best score:\", best_score_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.071\n",
      "Testing Accuracy: 0.014 \n",
      "\n",
      "R-squared: 0.149\n",
      "MAPE: 0.222\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the training and testing datasets using the best KNN classifier model\n",
    "y_train_pred = search2.predict(X_train)\n",
    "y_test_pred = search2.predict(X_test)\n",
    "\n",
    "# Create confusion matrices for training and testing datasets\n",
    "confusion_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "confusion_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Calculate accuracy for training and testing datasets\n",
    "accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the training and testing accuracies\n",
    "print(\"Training Accuracy:\", round(accuracy_train, 3))\n",
    "print(\"Testing Accuracy:\", round(accuracy_test, 3), \"\\n\")\n",
    "\n",
    "# Calculate R-squared (coefficient of determination) for KNN classifier predictions\n",
    "knn_r2 = r2_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "# Print the R-squared score\n",
    "print(\"R-squared:\", round(knn_r2, 3))\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for KNN classifier predictions\n",
    "knn_mape = mean_absolute_percentage_error(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "# Print the MAPE score\n",
    "print(\"MAPE:\", round(knn_mape, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full pipeline that includes preprocessing steps and a RandomForestRegressor\n",
    "full_pipeline = make_pipeline(preprocessor, RandomForestRegressor())\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'columntransformer__num_pipe__selectkbest__k': [19],  # Number of top features to select\n",
    "    'columntransformer__num_pipe__simpleimputer__strategy': ['median'],  # Impute missing values with the median\n",
    "    'columntransformer__num_pipe__variancethreshold__threshold': [0.005],  # Threshold for removing low-variance features\n",
    "    'randomforestregressor__criterion': ['poisson'],  # Criterion for splitting nodes\n",
    "    'randomforestregressor__max_depth': [8],  # Maximum depth of the trees\n",
    "    'randomforestregressor__max_features': ['auto'],  # Maximum number of features to consider for splitting\n",
    "    'randomforestregressor__n_estimators': [200]  # Number of trees in the forest\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object to search for the best hyperparameters\n",
    "search3 = GridSearchCV(full_pipeline, param_grid, cv=10, verbose=0)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "search3.fit(X_train, y_train)\n",
    "\n",
    "# Get the best cross-validation score from the grid search for RandomForestRegressor\n",
    "best_score_rf = search3.best_score_\n",
    "\n",
    "# Store the best score in a dictionary\n",
    "scores3 = {\"rf\": best_score_rf}\n",
    "\n",
    "# Print the best parameters and corresponding score\n",
    "print(\"Best parameters: \", search3.best_params_)\n",
    "print(scores3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest - Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.872\n",
      "MAPE: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the training and testing datasets using the best RandomForestRegressor model\n",
    "y_train_pred = search3.predict(X_train)\n",
    "y_test_pred = search3.predict(X_test)\n",
    "\n",
    "# Calculate R-squared (coefficient of determination) for RandomForestRegressor predictions\n",
    "rf_r2 = r2_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "# Print the R-squared score\n",
    "print(\"R-squared:\", round(rf_r2, 3))\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for RandomForestRegressor predictions\n",
    "rf_mape = mean_absolute_percentage_error(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "# Print the MAPE score\n",
    "print(\"MAPE:\", round(rf_mape, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreesRegressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full pipeline that includes preprocessing steps and an ExtraTreesRegressor\n",
    "full_pipeline = make_pipeline(preprocessor, ExtraTreesRegressor())\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'columntransformer__num_pipe__simpleimputer__strategy': ['mean'],  # Impute missing values with the mean\n",
    "    'columntransformer__num_pipe__selectkbest__k': [15],  # Number of top features to select\n",
    "    'columntransformer__num_pipe__variancethreshold__threshold': [0.005],  # Threshold for removing low-variance features\n",
    "    'extratreesregressor__n_estimators': [200],  # Number of trees in the forest\n",
    "    'extratreesregressor__max_depth': [8],  # Maximum depth of the trees\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object to search for the best hyperparameters\n",
    "ert_search = GridSearchCV(full_pipeline, param_grid, cv=5, verbose=1)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "ert_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best cross-validation score from the grid search for ExtraTreesRegressor\n",
    "best_score_ert = ert_search.best_score_\n",
    "\n",
    "# Store the best score in a dictionary\n",
    "scores = {\"ERT\": best_score_ert}\n",
    "\n",
    "# Print the best parameters and corresponding score\n",
    "print(\"Best parameters: \", ert_search.best_params_)\n",
    "print(\"Best score:\", best_score_ert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.095\n",
      "R-squared: 0.887\n"
     ]
    }
   ],
   "source": [
    "# Create an ExtraTreesRegressor pipeline that includes preprocessing steps\n",
    "ETR_pipeline = make_pipeline(preprocessor, ExtraTreesRegressor())\n",
    "\n",
    "# Fit the ExtraTreesRegressor pipeline on the training data\n",
    "ETR_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing dataset using the trained ExtraTreesRegressor model\n",
    "ert_predictions = ETR_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for ExtraTreesRegressor predictions\n",
    "ERT_mape = mean_absolute_percentage_error(y_true=y_test, y_pred=ert_predictions)\n",
    "\n",
    "# Print the MAPE score\n",
    "print(\"MAPE:\", round(ERT_mape, 3))\n",
    "\n",
    "# Calculate R-squared (coefficient of determination) for ExtraTreesRegressor predictions\n",
    "ERT_r2 = r2_score(y_true=y_test, y_pred=ert_predictions)\n",
    "\n",
    "# Print the R-squared score\n",
    "print(\"R-squared:\", round(ERT_r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.115\n",
      "R-squared: 0.851\n"
     ]
    }
   ],
   "source": [
    "# Create an SGD Regressor pipeline that includes preprocessing steps\n",
    "sgd_pipeline = make_pipeline(preprocessor, SGDRegressor())\n",
    "\n",
    "# Fit the SGD Regressor pipeline on the training data\n",
    "sgd_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing dataset using the trained SGD Regressor model\n",
    "sgd_predictions = sgd_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for SGD Regressor predictions\n",
    "SGD_mape = mean_absolute_percentage_error(y_true=y_test, y_pred=sgd_predictions)\n",
    "\n",
    "# Print the MAPE score\n",
    "print(\"MAPE:\", round(SGD_mape, 3))\n",
    "\n",
    "# Calculate R-squared (coefficient of determination) for SGD Regressor predictions\n",
    "SGR_r2 = r2_score(y_true=y_test, y_pred=sgd_predictions)\n",
    "\n",
    "# Print the R-squared score\n",
    "print(\"R-squared:\", round(SGR_r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.111\n",
      "R-squared: 0.881\n"
     ]
    }
   ],
   "source": [
    "# Create a Linear Regression pipeline that includes preprocessing steps\n",
    "lr_pipeline = make_pipeline(preprocessor, LinearRegression())\n",
    "\n",
    "# Fit the Linear Regression pipeline on the training data\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing dataset using the trained Linear Regression model\n",
    "lr_predictions = lr_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE) for Linear Regression predictions\n",
    "lr_mape = mean_absolute_percentage_error(y_true=y_test, y_pred=lr_predictions)\n",
    "\n",
    "# Print the MAPE score\n",
    "print(\"MAPE:\", round(lr_mape, 3))\n",
    "\n",
    "# Calculate R-squared (coefficient of determination) for Linear Regression predictions\n",
    "lr_r2 = r2_score(y_true=y_test, y_pred=lr_predictions)\n",
    "\n",
    "# Print the R-squared score\n",
    "print(\"R-squared:\", round(lr_r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision_tree</th>\n",
       "      <th>knn</th>\n",
       "      <th>RF</th>\n",
       "      <th>SGDR</th>\n",
       "      <th>LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.392</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          decision_tree    knn     RF  SGDR     LR\n",
       "baseline          0.392  0.149  0.876  0.85  0.881"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get predictions for different models on the testing dataset\n",
    "y_test_pred_tree = search.predict(X_test)  # Decision Tree\n",
    "y_test_pred_knn = search2.predict(X_test)  # K-Nearest Neighbors\n",
    "y_test_pred_rf = search3.predict(X_test)    # Random Forest\n",
    "y_test_pred_SGDR = sgd_pipeline.predict(X_test)  # Stochastic Gradient Descent Regressor\n",
    "y_test_pred_lr = lr_pipeline.predict(X_test)    # Linear Regression\n",
    "\n",
    "# Calculate R-squared scores for each model's predictions\n",
    "baseline_tree_r2 = r2_score(y_test, y_test_pred_tree)        # R-squared for Decision Tree\n",
    "baseline_knn_r2 = r2_score(y_test, y_test_pred_knn)          # R-squared for K-Nearest Neighbors\n",
    "baseline_rf = r2_score(y_test, y_test_pred_rf)               # R-squared for Random Forest\n",
    "baseline_SGDR = r2_score(y_test, y_test_pred_SGDR)           # R-squared for Stochastic Gradient Descent Regressor\n",
    "baseline_lr = r2_score(y_test, y_test_pred_lr)               # R-squared for Linear Regression\n",
    "\n",
    "# Create a DataFrame to compare the performance of different models\n",
    "performances = pd.DataFrame({'decision_tree': round(baseline_tree_r2, 3),\n",
    "                             'knn': round(baseline_knn_r2, 3), \n",
    "                             'RF': round(baseline_rf, 3),\n",
    "                             'SGDR': round(baseline_SGDR, 2),\n",
    "                             'LR': round(baseline_lr, 3)},\n",
    "                            index=['baseline'])\n",
    "\n",
    "# Print the DataFrame with R-squared scores\n",
    "print(performances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With out Threshold and Skaler\n",
    "\n",
    "| Model       | Decision Tree | KNN   | Random Forest | SGDR  | Logistic Regression |\n",
    "|-------------|---------------|-------|---------------|-------|----------------------|\n",
    "| Baseline    | 0.372         | -0.229 | 0.697         | -3.505539e+22  | 0.884               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Threshold and Skaler\n",
    "\n",
    "| Model       | Decision Tree | KNN   | Random Forest | SGDR  | Logistic Regression |\n",
    "|-------------|---------------|-------|---------------|-------|----------------------|\n",
    "| Baseline    | 0.454         | 0.108 | 0.595         | 0.84  | 0.866                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Threshold and Skaler and GridSearh\n",
    "\n",
    "| Model       | Decision Tree | KNN   | Random Forest | SGDR  | Logistic Regression |\n",
    "|-------------|---------------|-------|---------------|-------|----------------------|\n",
    "| Baseline    | 0.31           | 0.64 | 0.7           | 0.87  |   -7.169538e+19          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X_sumbmition as the test_housing_data\n",
    "X_sumbmition = test_housing_data\n",
    "\n",
    "# Use the trained ETR pipeline to predict SalePrice for X_sumbmition\n",
    "ids_com['SalePrice'] = ETR_pipeline.predict(X_sumbmition)\n",
    "\n",
    "# Save the predictions to a CSV file named 'submission_ert_old.csv'\n",
    "ids_com.to_csv(r'submission_ert_old.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
